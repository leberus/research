	* TLB [http://pages.cs.wisc.edu/~remzi/OSTEP/vm-tlbs.pdf]

                * Sometimes implemented as Content-addressable memory
                        - Virtual address  -> CAM search key
                        - Physical address -> CAM search result

                * If requested address is not in TLB:
                        * TLB does a pagewalk to find the mapping
                        * Mapping is entered into the TLB

                * It's a cache of the page table, representing only a subset of the page table contents

                * Has a fixed number of slots containing page table entries
                        * page table entries map virtual address to physical address and intermediate table address.

                * May reside between:
                        * CPU and the CPU cache:
                                * cache is physically addressed
                                * CPU does a TLB lookup on every memory operation and the resulting physical address
                                int sent to the cache
				* slow things down

                        * CPU cache and primary storage memory:
				* cache is virtually addressed
                                * requests are sent directly from the CPU to the cache, and the TLB is accessed
                                only on a cache miss


                * Steps in a TLB:
                        https://upload.wikimedia.org/wikipedia/commons/c/c1/Steps_In_a_Translation_Lookaside_Buffer.png

                * If the page working set does not fit into the TLB, then TLB trashing occurs,
                where frequent TLB misses occur, with each newly cached page displacing one that
                will soon be used again.

                * Working set [https://en.wikipedia.org/wiki/Working_set]

                * There can be multiple TLBs

                * TLB miss handling can be carried out by:
                        * hardware-TLB management  CPU walks the page tables, if can't find anything,
                        it raises a page-fault exception.
                        * software-TLB management: TLB miss generates an exception that is being catched by the OS

                * Typical performance levels of TLB:
                        - size: 12–4,096 entries
                        - hit time: 0.5–1 clock cycle
                        - miss penalty: 10–100 clock cycles
                        - miss rate: 0.01–1% (20-40% for sparse/graph applications)

                        If a TLB hit takes 1 clock cycle, a miss takes 30 clock cycles,
                        and the miss rate is 1%, the effective memory cycle rate is
                        an average of 1 × 0.99 + (1 + 30) × 0.01 = 1.30 (1.30 clock cycles per memory access).


                * TLB gets flushed on a process switch, so any memory reference will be a miss, and it'll
                take some time until TLB gets populated again.
                * To workaround this:
                        * in some hardware a TLB entry is tagged with address space number,
                        and only TLB entries with the current ASN are considered valid.
                        * the page global enable flag in the register CR4 and the global flag of a
                        page-directory or page-table entry can be used to prevent frequently used pages
                        from being automatically invalidated on a task switch/load of register C4.
                        * Some hardware allows flusing of individial entries from the BLX indexed by virtual address.


		* Example / PoC:

			Address space: 256 bytes
			Pages size: 16 bytes
			Number pages: 256 /16 = 16pages (Virtual Page Number)
			Array: 10 integers of 4 bytes

				Offset within a page
			[00]	[04]	[08]	[12]	[16]
	VPN	[00]
	VPN	[01]
	VPN	[02]
	...
	...
	VPN	[06]		    a[0]    a[1]    a[2]
	VPN	[07]	    a[3]    a[4]    a[5]    a[6]
	VPN	[08]	    a[7]    a[8]    a[9]
	...
	...
	VPN	[15]
	VPN	[16]
	

	
			a[10] = {0, 4, 8, 2, ...};
			int sum = 0;
			for (i = 0; i < 10 ; i++)
				sum += a[i];

			- a[0] = miss (first time program accesses the array)
			- a[1], a[2] = hit (a[1] and a[2] are next to a[0], spatial locality)
			- a[3] = miss
			- a[4], a[5], a[6] = hit (a[4], a[5], a[6] are next to a[3], spatial locality)
			- a[7] = miss
			- a[8], a[9] = hit (a[8], a[9] are next to a[7], spatial locality)

			* First reference of a virtual page hits a miss.
			  But then, all reference to elements within that page
			  cause a hit, because the translation for that virtual page was
                          already cached in the first access. (spatial locality)

			* if the program runs again, quick re-referencing of
			  memory items would have a big performance (since the VPN are now cached)
			  Data or instruction that has been recently accessed will likely
			  to be re-accessed soon in the future. (temporary locality)

			- temporary locality:
				* if an instruction or data item has been recently accessed,
				it'll likely be re-accessed soon in the future.

			- spatial locality:
				* if a program accesses memory at address x, it'll likely soon
				access memory near x.


	* TLBi? TLBd?


        * Content-addressable memory [https://en.wikipedia.org/wiki/Content-addressable_memory]:

                * Hardware associative array
                * User supplies a data word and the CAM searches its entire memory to see if that
                word is stored in it.
                If it's found, the CAM returns a list storage addresses where the word was found.
                * Used where searching speed cannot be accomplished using a less costly method.

                * Binary CAM: data search words consisting of 1s and 0s
                * Ternary CAM: data search words consisting 1s, 0s and "x - don't care bit".


	* Skylake Architecture example:

		2.1.3 
		Cache and Memory Subsystem
		The cache hierarchy of the Skylake microarchitecture has the following enhancements:
		• Higher Cache bandwidth compared to previous generations. 
		• Simultaneous handling of more loads and stores enabled by enlarged buffers. 
		• Processor can do two page walks in parallel compared to one in Haswell microarchitecture and earlier 
		generations. 
		• Page split load penalty down from 100 cycles in previous generation to 5 cycles. 
		• L3 write bandwidth increased from 4 cycles per line in previous generation to 2 per line. 
		• Support for the CLFLUSHOPT instruction to flush cache lines and manage memory ordering of flushed 
		data using SFENCE. 
		• Reduced performance penalty for a software prefetch that specifies a NULL pointer. 
		• L2 associativity changed from 8 ways to 4 ways



[links]

https://cs.stackexchange.com/questions/18313/how-does-a-tlb-and-data-cache-work [TLB + data cache]
http://pages.cs.wisc.edu/~remzi/OSTEP/vm-tlbs.pdf			[TLB: Faster Translations]
https://software.intel.com/sites/default/files/managed/7c/f1/253668-sdm-vol-3a.pdf
https://www.kernel.org/doc/Documentation/cachetlb.txt
https://lwn.net/Articles/379748/
https://en.wikibooks.org/wiki/QEMU/Monitor
http://research.cs.wisc.edu/multifacet/papers/can14_badgertrap.pdf 	[A tool to instrument x86-64 TLB misses]


http://www.cs.rochester.edu/~sandhya/csc256/seminars/vm_yuxin_yanwei.pdf ?

[info]
qemu memory and tlb

virsh qemu-monitor-command --hmp suma31pg 'info mem'
virsh qemu-monitor-command --hmp suma31pg 'info tlb'

